from typing import TypedDict, Any
from app.services.vectordb_service import search
from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph
from langgraph.constants import END

# This service will define the State, Nodes, and Graph for our LangGraph implementation

# Define the LLM we're going to use 
llm = ChatOllama(
    model="llama3.2:3b", # The model we're using (we installed llama3.2:3b)
    temperature=0.2 # Temp goes from 0-1. Higher temp = more creativity
)
# First, we'll define the State of our Graph 
# You can think of State like a container for global data. The "state" of the app 

# Each node in the graph can read from and write to State 

class GraphState(TypedDict, total = False): #total =False makes all fields optional 

    # The most recent user query 
    query: str 

    # The "routing decision" we make. This tells the app what node to invoke next 
    route: str 

    # Any retrieve documents from the Vector DB 
    docs: list[dict[str, Any]]

    # The final answer generated by the LLM
    answer: str 

# ===============================(NODE DEFINITIONS)==================================

# Notice each node takes in graph state AND returns graph state
# Nodes have access to the entire state AND can modify it 

# The Route Node - decides what node to invoke based on the user's query 
# This is a very user-facing node. It's the first to read user's query 
def route_node(state: GraphState) -> GraphState:
    
    # Get the user's query, turn it to lowercase for each of keyword matching
    query = state.get("query", "").lower()
    # WHAT is this? We're getting the user's query out of the state "qeury" field
    # How does state already know what the query is?
    # The user's query gets passed in when the graph is invoked and GraphState is initialized 

    # VERY basic keyword matching (for now) to decide which node to invoke next 
    # TODO: Use the LLM to decide which node to invoke instead of using hardcoded keyword matching 

    if any(word in query for word in ["item", "items", "product", "buy", "purchase"]):
        return {"route": "items"} 
    
    if any(word in query for word in ["boss", "boss's", "plan", "plans", "scheme", "schemes"]):
        return {"route": "plans"}
    
    # TODO: default chat node if routes are identified 

#The node that pulls from the "evil_items" collection in our Chroma Store 
def extract_items_node(state: GraphState) -> GraphState:
    # Simply search the VectorDB "evil_items" collection with the user's query in state 
    query = state.get("query", "")
    results = search(query, k=5, collection="evil_items")

    # Return the documents, adding them to state 
    return {"docs": results}

# The node that pulls from the "boss_plans" collection in our Chroma Store 
def extract_plans_node(state: GraphState) -> GraphState:
    #Same pattern as the node above 

    query = state.get("query", "")
    results = search(query, k=10, collection="boss_plans")
    return {"docs": results}

# The node that answers the user's query based on docs retrieved from either "extract" node 
def answer_with_context_node(state: GraphState) -> GraphState:
    # This should be a pretty comfortable pattern - just talking to the LLM 

    #First, extract the query and docs from state
    query = state.get("query", "")
    docs = state.get("docs", [])
    combined_docs = combined_docs = "\n\n".join(item["text"] for item in docs)

    # Set up a prompt
    prompt = (
        f"You are an internal assistant at the Evil Scientist Corp."
        f"You are pretty evil yourself, but still helpful."
        f"Answer the User's query based ONLY on the extracted data below."
        f"If the data doesn't help, say you don't know."
        f"Extracted Data: \n{combined_docs}"
        f"User Query: \n{query}"
        f"Answer: "
    )

    # Invoke the LLM with the prompt
    response = llm.invoke(prompt)

    # Return the answer, which also adds it to state 
    return {"answer": response}
# ===============================(END OF NODE DEFINITIONS)==================================

# Define the function that builds and returns the graph 
# In our endpoint, we'll invoke the graph, not the LLM directly, definitely not a chain 

def build_graph():

    # First, define the graph builder using our state
    build = StateGraph(GraphState)

    # Register each node in the graph
    build.add_node("route", route_node)
    build.add_node("extract_items", extract_items_node)
    build.add_node("extract_plans", extract_plans_node)
    build.add_node("answer_with_context_node", answer_with_context_node)

    # Route node goes first, determining which node to hit based on user query 
    build.set_entry_point("route")

    # This ugly block of code executed whichever node we're routing to 
    build.add_conditional_edges(
        "route",
        # This function returns the key we use to choose an edge
        # This lambdda syntax lets us avoid making a whole extra function 

        lambda state: state["route"],

        # Map that key to the next node 
        {
            "plans": "extract_plans",
            "items": "extract_items"
        }


    )
    # After either retrieval node runs, invoke the answer node 
    build.add_edge("extract_plans", "answer_with_context_node")
    build.add_edge("extract_items", "answer_with_context_node") 

    # Define the END point of the graph 
    build.set_finish_point("answer_with_context_node")

    # Compile and create the invokable graph object. We invoke this in our endpoint! 
    return build.compile()

# Make a single graph instance (singleton) - ensure only one instance of the graph exists 
langgraph = build_graph()